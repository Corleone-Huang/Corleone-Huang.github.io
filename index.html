
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML>
<HEAD><META content="IE=7.0000" http-equiv="X-UA-Compatible">
<TITLE>Mengqi Huang - USTC</TITLE>
<META name=description
content="Mengqi Huang, Ph.D. , University of Science and Technology of China. ">
<META name=keywords
content="mengqi huang, ustc, university of science and technoloy of china, 
deep learning, image generation, cross-modality learning">
<META content=text/html;charset=utf-8 http-equiv=Content-Type>
<link rel="stylesheet" type="text/css" href="template.css" />
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-39532305-5', 'auto');
  ga('send', 'pageview');

</script>

</HEAD>
<link rel="shortcut icon" href="./pics/ustc-favicon.png"/>

<BODY>

<div id="header">
<img style="margin-bottom:10pt;margin-top:10pt;margin-left:10%;" src="./pics/ustc-logo.png" height="67">
</div>

<DIV id=content>


<DIV id=left>
<table style="background-color:white;">
<tr NOSAVE>
<td VALIGN=CENTER>
<!-- <img SRC="./pics/huangmq.jpeg" height="260"align=right> -->
<img SRC="pics/self.jpg" height="260" align=center>
</td>

<td VALIGN=CENTER WIDTH="2%">
</td>

<td VALIGN=CENTER HALIGN=LEFT>
<font size=+0>
<b><font size=+2>Mengqi Huang&nbsp;</font></b>
<p style="margin-left:0px;">
<img SRC="./pics/huangmq_name.png" height="64">
<p style="margin-left:0px;">
<b>Ph.D. </b>
<p style="margin-left:0px;">
<!-- School of Cyber Science and Technology<br> -->
University of Science and Technology of China<br>
<p style="margin-left:0px;">
443 Huangshan Road, Hefei, China 230027<br>
<p style="margin-left:0px;">
Email: huangmq@mail.ustc.edu.cn<br>
<p>
</font>
</td>
</tr>
</table>

<DIV style="margin-top:20px;">
<P>
<BR> I received my Ph.D. degree from the University of Science and Technology of China (USTC) in 2025. My research interests include deep generative models, image/video generation, and unified multimodal generation. I am the recipient of the Best Student Paper Award at ACM Multimedia 2022 as the first author. I have also received funding from the First National Natural Science Foundation of China for Youth Student Fundamental Research (Ph.D. student).
<BR> 
<BR> 2025年于中国科学技术大学获博士学位。博士期间以第一作者获CCF-A类国际会议ACM Multimedia 2022最佳学生论文奖。获批首届国家自然科学基金青年学生基础研究项目(博士研究生)，入选首届中国科协青年人才托举工程博士生专项(中国电子学会托举)。获2025年中国科学院院长特别奖。毕业曾入选阿里星(the 2025 Alistar Program - P7)、字节跳动头部人才计划等。
</DIV> 

<h2 style="CLEAR: both;">Education</h2>
<TABLE><TBODY><TR><TD class=left>
<A href="./papers/to-appear.html"><IMG src="./pics/ustc-logo-single.png", height="24"><BR></A></TD>
<TD><SPAN class=title>University of Science and Technology of China</SPAN>
<BR>Ph.D. in Cyber Security
<BR>
September 2020 - June 2025
<BR>
Advisor: <a href="https://scholar.google.com.hk/citations?user=m-0P8sgAAAAJ&hl=zh-CN&oi=ao" target="_blank">Prof. Zhendong Mao</a>
</TD></TR></TBODY></TABLE>

<TABLE><TBODY><TR><TD class=left>
<A href="./papers/to-appear.html"><IMG src="./pics/ustc-logo-single.png", height="24"><BR></A></TD>
<TD><SPAN class=title>University of Science and Technology of China</SPAN>
<BR>B.Eng in Automation 
  <BR> 
September 2016 - June 2020
<BR>
Advisor: <a href="https://scholar.google.com.hk/citations?user=m-0P8sgAAAAJ&hl=zh-CN&oi=ao" target="_blank">Prof. Zhendong Mao</a>
</TD></TR></TBODY></TABLE>


<h2 style="CLEAR: both;">Funding</h2>
<TABLE><TBODY><TR><TD class=left>
<A href="./papers/to-appear.html"><IMG src="./pics/National_Natural_Science_Foundation_of_China_logo.png", height="24"><BR></A></TD>
<TD><SPAN class=title>The National Natural Science Foundation of China for Youth Student Fundamental Research (Ph.D. Student)</SPAN>
<BR>
	Research on Text-Guided Long Video Generation based on Spatiotemporal Native Collaboration 
<BR>
	May 2024 - December 2026
<BR>
</TD></TR></TBODY></TABLE>


<H2>Selected Paper Publications [<A href="https://scholar.google.com.hk/citations?user=BJvM6gsAAAAJ&hl=zh-CN&oi=sra">Google Scholar<A>]</H2>

<br>
	<b>In the Year of 2025:</b>
<br>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://pubmed.ncbi.nlm.nih.gov/41105542/"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> RealCustom++: Representing Images as Real Textual Word for Real-Time Customization</SPAN>
		<BR> Zhendong Mao, <b>Mengqi Huang</b>, Fei Ding, Mingcong Liu, Qian He, Yongdong Zhang<BR>
		<b><font color="blue">T-PAMI</font> </b>, CCF-A
			<a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a> 
			<a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a> 
		<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://arxiv.org/pdf/2506.00512"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> Pro3D-Editor: A Progressive-Views Perspective for Consistent and Precise 3D Editing</SPAN>
		<BR> Yang Zheng, <b>Mengqi Huang📧(Corresponding Author)</b>, Nan Chen, Zhendong Mao<BR>
		<b><font color="blue">NeurIPS 2025</font> </b>, CCF-A
			<a href="https://github.com/shuoyueli4519/Pro3D-Editor-Code"><img alt="Build" src="https://img.shields.io/github/stars/shuoyueli4519/Pro3D-Editor-Code"></a> 
			<a href="https://shuoyueli4519.github.io/Pro3D-Editor/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-Pro3D-yellow"></a> 
		<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://arxiv.org/pdf/2504.02160"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</SPAN>
		<BR> Shaojin Wu, <b>Mengqi Huang📧(Corresponding Author)</b>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He<BR>
		<b><font color="blue">ICCV 2025</font> </b>, CCF-A
			<!-- [<A href="https://github.com/bytedance/UNO">Github<A>] -->
			<a href="https://github.com/bytedance/UNO"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/UNO"></a> 
			<a href="https://bytedance.github.io/UNO/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a> 
		<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://arxiv.org/pdf/2503.10406"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models</SPAN>
		<BR> Yijing Lin, <b>Mengqi Huang</b>, Shuhan Zhuang, Zhendong Mao<BR>
		<b><font color="blue">ICCV 2025</font> </b>, CCF-A
			<a href="https://github.com/Lyne1/Realgeneral"><img alt="Build" src="https://img.shields.io/github/stars/Lyne1/Realgeneral"></a> 
			<a href="https://lyne1.github.io/realgeneral_web/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealGeneral-yellow"></a> 
		<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://arxiv.org/pdf/2505.02192"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</SPAN>
		<BR> Wenchuan Wang, <b>Mengqi Huang</b>, Yijing Tu, Zhendong Mao<BR>
		<b><font color="blue">ICCV 2025</font> </b>, CCF-A
			<a href="https://github.com/wenc-k/DualReal"><img alt="Build" src="https://img.shields.io/github/stars/wenc-k/DualReal"></a> 
			<a href="https://wenc-k.github.io/dualreal-customization/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-DualReal-yellow"></a> 
		<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://arxiv.org/pdf/2507.01945"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> LongAnimation: Long Animation Generation with Dynamic Global-Local Memory</SPAN>
		<BR> Nan Chen, <b>Mengqi Huang</b>, Yihao Meng, Zhendong Mao<BR>
		<b><font color="blue">ICCV 2025</font> </b>, CCF-A
		<!-- [<A href="https://github.com/CN-makers/LongAnimation">Github<A>] -->
			<a href="https://github.com/CN-makers/LongAnimation"><img alt="Build" src="https://img.shields.io/github/stars/CN-makers/LongAnimation"></a> 
			<a href="https://cn-makers.github.io/long_animation_web/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-LongAnimation-yellow"></a> 
		<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://openaccess.thecvf.com/content/CVPR2025/papers/Tu_A4A_Adapter_for_Adapter_Transfer_via_All-for-All_Mapping_for_Cross-Architecture_CVPR_2025_paper.pdf"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> A4A: Adapter for Adapter Transfer via All-for-All Mapping for Cross-Architecture Models</SPAN>
		<BR> Keyu Tu, <b>Mengqi Huang</b>, Zhuowei Chen, Zhendong Mao<BR>
		<b><font color="blue">CVPR 2025</font> </b>, CCF-A<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://openaccess.thecvf.com/content/CVPR2025/papers/Jia_D2iT_Dynamic_Diffusion_Transformer_for_Accurate_Image_Generation_CVPR_2025_paper.pdf"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation</SPAN>
		<BR> Weinan Jia, <b>Mengqi Huang</b>, Nan Chen, Lei Zhang, Zhendong Mao<BR>
		<b><font color="blue">CVPR 2025</font> </b>, CCF-A<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://openaccess.thecvf.com/content/CVPR2025/papers/Guang_Dragin3D_Image_Editing_by_Dragging_in_3D_Space_CVPR_2025_paper.pdf"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> Dragin3D: Image Editing by Dragging in 3D Space </SPAN>
		<BR> Weiran Guang, Xiaoguang Gu, <b>Mengqi Huang</b>, Zhendong Mao<BR>
		<b><font color="blue">CVPR 2025</font> </b>, CCF-A<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_FeedEdit_Text-Based_Image_Editing_with_Dynamic_Feedback_Regulation_CVPR_2025_paper.pdf"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> FeedEdit: Text-Based Image Editing with Dynamic Feedback Regulation </SPAN>
		<BR> Fengyi Fu, Lei Zhang, <b>Mengqi Huang</b>, Zhendong Mao<BR>
		<b><font color="blue">CVPR 2025</font> </b>, CCF-A<BR>
	</TD>
</TR></TBODY></TABLE>

<TABLE><TBODY><TR>
	<TD class=left><A href="https://ojs.aaai.org/index.php/AAAI/article/view/32210/34365"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD>
	<TD>
		<SPAN class=title> CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven Text-to-Image Customization</SPAN>
		<BR> Nan Chen, <b>Mengqi Huang</b>, Zhuowei Chen, Yang Zheng, Lei Zhang, Zhendong Mao<BR>
		<b><font color="blue">AAAI 2025</font> </b>, CCF-A<BR>
	</TD>
</TR></TBODY></TABLE>

<br>
	<b>In the Year of 2024:</b>
<br>

<TABLE>
	<TBODY>
		<TR>
			<TD class=left>
				<A href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf"><IMG src="./pics/pdf.gif"><BR>PDF</A>
			</TD>
			<TD>
				<SPAN class=title> RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization
				</SPAN>
				<BR> 
					<b>Mengqi Huang</b>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang
				<BR>
					<b><font color="blue">CVPR 2024</font> </b>, CCF-A
					<!-- [<A href="https://corleone-huang.github.io/realcustom/">Project Page<A>] -->
					<!-- [<A href="https://github.com/bytedance/RealCustom">Github<A>] -->
					<a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a> 
					<a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a> 
				<BR>
			</TD>
		</TR>
	</TBODY>
</TABLE>

<TABLE>
	<TBODY>
		<TR>
			<TD class=left>
				<A href="https://ojs.aaai.org/index.php/AAAI/article/view/28089"><IMG src="./pics/pdf.gif"><BR>PDF</A>
			</TD>
			<TD>
				<SPAN class=title> Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing
				</SPAN>
				<BR>
					Hao Li, <b>Mengqi Huang</b>, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao
				<BR> 
					<b><font color="blue">AAAI 2024</font> </b>, CCF-A
			</TD>
		</TR>
	</TBODY>
</TABLE>

<TABLE>
	<TBODY>
		<TR>
			<TD class=left>
				<A href="https://ojs.aaai.org/index.php/AAAI/article/view/27891"><IMG src="./pics/pdf.gif"><BR>PDF</A>
			</TD>
			<TD>
				<SPAN class=title> DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation
				</SPAN>
				<BR>
					Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, <b>Mengqi Huang</b>, Yongdong Zhang, Zhendong Mao
				<BR> 
					<b><font color="blue">AAAI 2024</font> </b>, CCF-A
					<!-- [<A href="https://dreamidentity.github.io/">Project Page<A>] -->
					<a href="https://dreamidentity.github.io/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-DreamIdentity-yellow"></a> 
			</TD>
		</TR>
	</TBODY>
</TABLE>

<br>
	<b>In the Year of 2023 & 2022:</b>
<br>

<TABLE>
	<TBODY>
		<TR>
			<TD class=left>
				<A href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf"><IMG src="./pics/pdf.gif"><BR>PDF</A>
			</TD>
			<TD>
				<SPAN class=title>Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization 
					<font color="red">(Highlight, 2.6% of submitted papers)</font> 
				</SPAN>
				<BR>
					<b>Mengqi Huang</b>, Zhendong Mao, Zhuowei Chen, Yongdong Zhang
				<BR> 
					<b><font color="blue">CVPR 2023</font> </b>, CCF-A
					<a href="https://github.com/CrossmodalGroup/DynamicVectorQuantization"><img alt="Build" src="https://img.shields.io/github/stars/CrossmodalGroup/DynamicVectorQuantization"></a> 
					<a href="https://youtu.be/ir60YW9JCjU"><img src="https://img.shields.io/badge/Video-DynamicVQ-red?logo=youtube"></a>
					<!-- [<A href="https://github.com/CrossmodalGroup/DynamicVectorQuantization">Github<A>]
					[<A href="https://youtu.be/ir60YW9JCjU">Video<A>] -->
			</TD>
		</TR>
	</TBODY>
</TABLE>

<TABLE>
	<TBODY>
		<TR>
			<TD class=left>
				<A href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf"><IMG src="./pics/pdf.gif"><BR>PDF</A>
			</TD>
			<TD>
				<SPAN class=title>Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation
				</SPAN>
				<BR>
					<b>Mengqi Huang</b>, Zhendong Mao, Quan Wang, Yongdong Zhang
				<BR> 
					<b><font color="blue">CVPR 2023</font> </b>, CCF-A
					<a href="https://github.com/CrossmodalGroup/MaskedVectorQuantization"><img alt="Build" src="https://img.shields.io/github/stars/CrossmodalGroup/MaskedVectorQuantization"></a> 
					<a href="https://www.youtube.com/watch?v=o2eyRscEejw"><img src="https://img.shields.io/badge/Video-MaskedVQ-red?logo=youtube"></a>
					<!-- [<A href="https://github.com/CrossmodalGroup/MaskedVectorQuantization">Github<A>] 
					[<A href="https://www.youtube.com/watch?v=o2eyRscEejw">Video<A>] -->
			</TD>
		</TR>
	</TBODY>
</TABLE>

<TABLE>
	<TBODY>
		<TR>
			<TD class=left>
				<A href="https://arxiv.org/abs/2209.01339"><IMG src="./pics/pdf.gif"><BR>PDF</A>
			</TD>
			<!-- <TD>
				<img src="./pics/mm_award.jpg" width="100%">
			</TD> -->
			<TD>
				<SPAN class=title>DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation 
					<font color="red">(Best Student Paper Award, 1/3009 of submitted papers)</font> </SPAN>
				</SPAN>
				<BR>
					<b>Mengqi Huang</b>, Zhendong Mao, Penghui Wang, Quan Wang, Yongdong Zhang
				<BR>
					<b><font color="blue">ACM Multimedia 2022</font> </b>, CCF-A
				<BR> 
					<div>
						<IMG src="./pics/mm_award_compressed.jpg", width="60%">
					</div>
			</TD>
		</TR>
	</TBODY>
</TABLE>
<!-- 2473 -->

<!-- <table>
	<tbody>
	<tr>
		<td class="pub-photo"><img src=DragGAN.gif width="100%"></td>
		<td>
		<span class="pub-title">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</span><br/>
		SIGGRAPH 2023 <br/>
		<span><a href="https://xingangpan.github.io//">Xingang Pan</a>, <span class="self-author">Ayush Tewari</span>, <a href="https://people.mpi-inf.mpg.de/~tleimkue/">Thomas Leimkuehler</a>, <a href="https://lingjie0206.github.io/">Lingjie Liu</a>, <a href="https://www.meka.page/">Abhimitra Meka</a>, and <a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a><br/>
		<a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/">[project page]</a> <a href="https://arxiv.org/abs/2305.10973">[paper]</a> <a href="https://github.com/XingangPan/DragGAN">[code]</a><br/>
		</td>
	</tr>
	</tbody>
</table> -->

<!-- <TABLE><TBODY><TR><TD class=left> -->
<!-- <A href="https://arxiv.org/abs/2209.01339"><IMG src="./pics/pdf.gif"><BR>PDF</A></TD> -->
<!-- <td class="pub-photo"><img src=./pics/mm_award.jpg width="100%"></td>
<TD><SPAN class=title>DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation <font color="red">[Best Student Paper Award, 1/2473 of submitted papers]</font> </SPAN>
<BR><b>Mengqi Huang</b>, Zhendong Mao, Penghui Wang, Quan Wang, Yongdong Zhang
<BR>ACM Multimedia (ACM MM) 2022, CCF-A, 27.9% acceptance rate -->
	<!-- <div>
		<IMG src="./pics/mm_award.jpg", height="280">
	</div> -->
<!-- </TD></TR></TBODY></TABLE> -->
<!-- (The 30th ACM International Conference on Multimedia Proceedings) -->

<h2 style="margin-bottom: 16px;">Awards</h2>
<ul style="
    list-style: disc inside;
    padding-left: 10px;
    font-size: 1.08em;
    line-height: 1.8;
    margin: 0;
    ">
  <li>中国科学院院长特别奖, 2025年</li>
  <li>Best Student Paper Award, ACM Multimedia 2022 (First Author)</li>
</ul>


<H2 style="CLEAR: both">Internships</H2>

<!-- <TABLE><TBODY><TR><TD class=left>
<IMG src="./pics/bytedance.jpeg" height="24"><BR></A></TD>
<TD><SPAN class=title> <A href="https://www.bytedance.com/zh/">ByteDance Inc.</A></SPAN>
<BR>Research Intern, Intelligent Creation Department.
<BR>Beijing 
<BR>July 2023 - Now
</TD></TR></TBODY></TABLE> -->

<TABLE><TBODY><TR>
	<TD class=left>
	  <IMG src="./pics/bytedance.jpeg" height="24"><BR>
	</TD>
	<TD>
	  <!-- 原实习基本信息 -->
	  <SPAN class=title> <A href="https://bytedance.com/">ByteDance Inc.</A></SPAN>
	  <BR>Research Intern, Intelligent Creation Department.
	  <BR>Beijing 
	  <BR>July 2023 - Now

<BR>
	  
<!-- 👇 项目模块：图在上，文在下 -->
  <DIV class="project-section">
	<DIV class="project-card">
	  <!-- 图片区 -->
	  <DIV class="project-image">
		<IMG src="./pics/teaser_realcustom.jpg" width="100%">
	  </DIV>
	  <!-- 文字区：标题+作者+链接+描述 -->
	  <DIV class="project-text">
		<DIV class="project-title">
		  RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization
		</DIV>
		<b><font color="blue">CVPR 2024 & T-PAMI</font> </b><BR>
		<DIV class="project-authors">
		  <strong>Mengqi Huang</strong>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang
		</DIV>
		<DIV class="project-links">
			<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/CVPR2024-RealCustom-b31b1b.svg"></a>
			<a href="https://pubmed.ncbi.nlm.nih.gov/41105542/"><img alt="Build" src="https://img.shields.io/badge/TPAMI-RealCustom++-b31b1b.svg"></a>
			<a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a> 
			<a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a> 
		  <!-- <A href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf">RealCustom</A> / 
		  <A href="Realcustom++: Representing images as real-word for real-time customization">RealCustom++</A> / 
		  <A href="https://corleone-huang.github.io/RealCustom_plus_plus/">Homepage</A> / 
		  <A href="https://github.com/bytedance/RealCustom">Code</A> /  -->
		</DIV>
		<DIV class="project-desc">
		  We present RealCustom to disentangle subject similarity from text controllability and thereby allows both to be optimized simultaneously without conflicts. The core idea of RealCustom is to represent given subjects as real words that can be seamlessly integrated with given texts, and further leveraging the relevance between real words and image regions to disentangle visual condition from text condition.
		</DIV>
	  </DIV>
	</DIV>
  </DIV>
<!-- 👆 项目模块结束 -->

<BR>

<!-- 👇 项目模块：图在上，文在下 -->
<DIV class="project-section">
	<DIV class="project-card">
	  <!-- 图片区 -->
	  <DIV class="project-image">
		<IMG src="./pics/teaser_uno.jpg" width="100%">
	  </DIV>
	  <!-- 文字区：标题+作者+链接+描述 -->
	  <DIV class="project-text">
		<DIV class="project-title">
			Less-to-More Generalization: Unlocking More Controllability by In-Context Generation
		</DIV>
		<b><font color="blue">ICCV 2025</font> </b><BR>
		<DIV class="project-authors">
			Shaojin Wu, <strong>Mengqi Huang📧(Corresponding Author)</strong>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He
		</DIV>
		<DIV class="project-links">
			<a href="https://arxiv.org/abs/2504.02160"><img alt="Build" src="https://img.shields.io/badge/arXiv%20paper-UNO-b31b1b.svg"></a>
			<a href="https://github.com/bytedance/UNO"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/UNO"></a>
			<a href="https://bytedance.github.io/UNO/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a> 
		  <!-- <A href="https://arxiv.org/pdf/2504.02160">UNO Paper</A> / 
		  <A href="https://github.com/bytedance/UNO">Code</A> /  -->
		</DIV>
		<DIV class="project-desc">
			We propose a highly-consistent data synthesis pipeline for single-subject and multi-subject driven generation. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Moreover, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding.
		</DIV>
	  </DIV>
	</DIV>
  </DIV>
<!-- 👆 项目模块结束 -->

	</TD>
  </TR></TBODY></TABLE>

<TABLE><TBODY><TR><TD class=left>
<IMG src="./pics/kwai-logo.jpeg" height="24"><BR></A></TD>
<!-- <TD><SPAN class=title> <A href="https://www.kuaishou.com/">Beijing Dajia Internet Information Technology Co., Ltd.</A></SPAN> -->
<TD><SPAN class=title> <A href="https://www.kuaishou.com/">Kuaishou Technology</A></SPAN>
<BR>Research Intern, Search Technology Department.
<BR>Beijing 
<BR>March 2022 - Novemeber 2022
</TD></TR></TBODY></TABLE>

<TABLE><TBODY><TR><TD class=left>
<IMG src="./pics/mcmaster.jpeg", height="24"><BR></A></TD>
<TD><SPAN class=title><A href="https://www.mcmaster.ca/">McMaster University</A></SPAN>
<BR>Research Intern, Computing & Software School (Department) 
<BR>Hamilton, Canada
<BR>June 2019 - September 2019
</TD></TR></TBODY></TABLE>


<H2 style="CLEAR: both">Competitions</H2>

<TABLE>
	<TBODY>
	<TR>
	  <TD><SPAN class=title><A
		href="https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af50464a0ed647faca6266&award=1,000,000">第二届粤港澳大湾区国际算法算例大赛-高效可控的文生图方法</A></SPAN> <BR>
		Team Leader, Second Prize.
		<BR>
		August 2023 - Novemeber 2023 <br>
		<!-- <div> -->
			<!-- <IMG src="./pics/xingzhi.jpg", width="80%"> -->
			<!-- <IMG src="pics/SuiBianTT-获奖证书.jpg", width="80%">
		</div> -->
    	<BR>
	  </TD>
	</TR>
	</TBODY>
</TABLE>

<TABLE>
	<TBODY>
	<TR>
	  <TD><SPAN class=title><A
		href="http://www.aiinnovation.com.cn/#/">首届兴智杯全国人工智能应用创新大赛-多模态技术创新赛-基于文本的图像生成</A></SPAN> <BR>
		Team Leader, Second Prize.
		<BR>
		August 2022 - Novemeber 2022 <br>
		<!-- <div> -->
			<!-- <IMG src="./pics/xingzhi.jpg", width="80%"> -->
			<!-- <IMG src="./pics/附件16-兴智杯-创新赛-二等奖证明.png", width="80%">
		</div> -->
    	<BR>
	  </TD>
	</TR>
	</TBODY>
</TABLE>

<TABLE>
	<TBODY>
	<TR>
	  <TD><SPAN class=title><A
		href="http://smp-challenge.com/">ACM Multimedia 2020 Social Media Prediction Challenge</A></SPAN> <BR>
		Team Leader, Top Performance Award. 
		[<A href="https://github.com/Corleone-Huang/Social-Media-Popularity-Prediction-Challenge-2020">Github<A>]
		<BR>
		March 2020 - June 2020 <br>
	<!-- <div>
		<IMG src="./pics/smp.jpg", width="80%">
	</div> -->
    <BR>
	  </TD></TR></TBODY></TABLE>

<!-- <TABLE>
<TBODY>
<TR>
	<TD><SPAN class=title><A
	href="https://iplab.dmi.unict.it/popularitychallenge/">ICIP 2020 Image Popularity Prediction Challenge</A></SPAN> <BR>
	Team Leader, Fourth Grade.
	<BR>
	March 2020 - June 2020 <br>
	</TD></TR></TBODY></TABLE> -->

<!-- <H2 style="CLEAR: both">Patents</H2>
<TABLE>
	<TBODY>
	<TR>
		<TD><SPAN class=title>基于迭代优化策略的多模态信息社交媒体流行度预测方法: 毛震东; 张勇东; 黄梦琪</SPAN> <BR>
		</TD></TR></TBODY></TABLE> -->

</br>
<a href="https://clustrmaps.com/site/1bpao"  title="Visit tracker">
	<div >
		<IMG src="http://clustrmaps.com/map_v2.png?d=x9zZiIk7-mY5aFcqqnSdkP6fOQeNgwkz4dAD8Esq9hE&cl=ffffff" height="100"/>
	</div>
</a>

<!-- <script type="text/javascript" id="clstr_globe" src="http://clustrmaps.com/globe.js?d=x9zZiIk7-mY5aFcqqnSdkP6fOQeNgwkz4dAD8Esq9hE"></script> -->

<p>Last update: October, 2025. Webpage template borrows from <a href="http://wnzhang.net/">Weinan Zhang</a>.</p>


</DIV>

<DIV id=news>
<H2>News</H2><BR>

<font size=3px>
	<B>October 2025</B><BR>
	<SPAN class=easylink>1 paper is accepted by T-PAMI!!</SPAN><BR><BR>
</font>

<font size=3px>
	<B>September 2025</B><BR>
	<SPAN class=easylink>1 paper is accepted by NeurIPS 2025!</SPAN><BR><BR>
</font>

<font size=3px>
	<B>June 2025</B><BR>
	<SPAN class=easylink>4 papers are accepted by ICCV 2025!</SPAN><BR><BR>
</font>

<font size=3px>
	<B>February 2025</B><BR>
	<SPAN class=easylink>4 papers are accepted by CVPR 2025!</SPAN><BR><BR>
</font>

<font size=3px>
	<B>December 2024</B><BR>
	<SPAN class=easylink>1 papers is accepted by AAAI 2025!</SPAN><BR><BR>
</font>

<font size=3px>
	<B>February 2024</B><BR>
	<SPAN class=easylink>1 papers is accepted by CVPR 2024!</SPAN><BR><BR>
</font>

<font size=3px>
	<B>December 2023</B><BR>
	<SPAN class=easylink>2 papers are accepted by AAAI 2024!</SPAN><BR><BR>
</font>

<font size=3px>
	<B>March 2023</B><BR>
	<SPAN class=easylink>Our paper "Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization" is selected as a <font color="red">highlight</font> at CVPR 2023!</SPAN><BR><BR>
</font>

<font size=3px>
<B>February 2023</B><BR>
<SPAN class=easylink>2 papers are accepted by CVPR 2023!</SPAN><BR><BR>
</font>

<font size=3px>
<B>October 2022</B><BR>
<SPAN class=easylink>Our paper "DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation" receives the <font color="red">Best Student Paper Award</font> at ACM Multimedia 2022!</SPAN><BR><BR>
</font>

<font size=3px>
<B>June 2022</B><BR>
<SPAN class=easylink>1 paper is accepted by ACM Multimedia 2022!</SPAN><BR><BR>
</font>


</DIV>


</DIV>


</BODY>
</HTML>
