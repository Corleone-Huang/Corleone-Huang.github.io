<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
    <meta content="IE=7.0000" http-equiv="X-UA-Compatible">
    <title>Mengqi Huang - USTC</title>
    <meta name="description" content="Mengqi Huang, Ph.D. , University of Science and Technology of China. ">
    <meta name="keywords" content="mengqi huang, ustc, university of science and technoloy of china, deep learning, image generation, cross-modality learning">
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <link rel="stylesheet" type="text/css" href="template.css" />
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-39532305-5', 'auto');
        ga('send', 'pageview');
    </script>
</head>
<link rel="shortcut icon" href="./pics/ustc-favicon.png"/>

<body>
    <div id="header">
        <img style="margin-bottom:10pt;margin-top:10pt;margin-left:10%;" src="./pics/ustc-logo.png" height="67">
    </div>

    <div id="content">
        <div id="left">
            <table style="background-color:white;">
                <tr NOSAVE>
                    <td valign="CENTER">
                        <img src="pics/self.jpg" height="260" align="center">
                    </td>

                    <td valign="CENTER" width="2%"></td>

                    <td valign="CENTER" halign="LEFT">
                        <font size="+0">
                            <b><font size="+2">Mengqi Huang&nbsp;</font></b>
                            <p style="margin-left:0px;">
                                <img src="./pics/huangmq_name.png" height="64">
                            <p style="margin-left:0px;">
                            <b>Ph.D. </b>
                            <p style="margin-left:0px;">
                                University of Science and Technology of China<br>
                            <p style="margin-left:0px;">
                                443 Huangshan Road, Hefei, China 230027<br>
                            <p style="margin-left:0px;">
                                Email: huangmq@mail.ustc.edu.cn<br>
                            <p>
                        </font>
                    </td>
                </tr>
            </table>

            <div style="margin-top:20px;">
                <p>
                <br> I received my Ph.D. degree from the University of Science and Technology of China (USTC) in 2025. My research interests include deep generative models, image/video generation, and unified multimodal generation. I am the recipient of the Best Student Paper Award at ACM Multimedia 2022 as the first author. I have also received funding from the First National Natural Science Foundation of China for Youth Student Fundamental Research (Ph.D. student).
                <br> 
                <br> 2025Âπ¥‰∫é‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶Ëé∑ÂçöÂ£´Â≠¶‰Ωç„ÄÇÂçöÂ£´ÊúüÈó¥‰ª•Á¨¨‰∏Ä‰ΩúËÄÖËé∑CCF-AÁ±ªÂõΩÈôÖ‰ºöËÆÆACM Multimedia 2022ÊúÄ‰Ω≥Â≠¶ÁîüËÆ∫ÊñáÂ•ñ„ÄÇËé∑ÊâπÈ¶ñÂ±äÂõΩÂÆ∂Ëá™ÁÑ∂ÁßëÂ≠¶Âü∫ÈáëÈùíÂπ¥Â≠¶ÁîüÂü∫Á°ÄÁ†îÁ©∂È°πÁõÆ(ÂçöÂ£´Á†îÁ©∂Áîü)ÔºåÂÖ•ÈÄâÈ¶ñÂ±ä‰∏≠ÂõΩÁßëÂçèÈùíÂπ¥‰∫∫ÊâçÊâò‰∏æÂ∑•Á®ãÂçöÂ£´Áîü‰∏ìÈ°π(‰∏≠ÂõΩÁîµÂ≠êÂ≠¶‰ºöÊâò‰∏æ)„ÄÇËé∑2025Âπ¥‰∏≠ÂõΩÁßëÂ≠¶Èô¢Èô¢ÈïøÁâπÂà´Â•ñ„ÄÇÊØï‰∏öÊõæÂÖ•ÈÄâÈòøÈáåÊòü(the 2025 Alistar Program - P7)„ÄÅÂ≠óËäÇË∑≥Âä®Â§¥ÈÉ®‰∫∫ÊâçËÆ°ÂàíÁ≠â„ÄÇ
            </div> 

            <h2 style="CLEAR: both;">Education</h2>
            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="./papers/to-appear.html"><img src="./pics/ustc-logo-single.png" height="24"><br></a>
                        </td>
                        <td>
                            <span class="title">University of Science and Technology of China</span>
                            <br>Ph.D.
                            <br>
                            September 2020 - June 2025
                            <br>
                            Advisor: <a href="https://scholar.google.com.hk/citations?user=m-0P8sgAAAAJ&hl=zh-CN&oi=ao" target="_blank">Prof. Zhendong Mao</a>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="./papers/to-appear.html"><img src="./pics/ustc-logo-single.png" height="24"><br></a>
                        </td>
                        <td>
                            <span class="title">University of Science and Technology of China</span>
                            <br>B.Eng
                            <br> 
                            September 2016 - June 2020
                            <br>
                            Advisor: <a href="https://scholar.google.com.hk/citations?user=m-0P8sgAAAAJ&hl=zh-CN&oi=ao" target="_blank">Prof. Zhendong Mao</a>
                        </td>
                    </tr>
                </tbody>
            </table>

            <h2 style="CLEAR: both;">Funding</h2>
            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="./papers/to-appear.html"><img src="./pics/National_Natural_Science_Foundation_of_China_logo.png" height="24"><br></a>
                        </td>
                        <td>
                            <span class="title">The National Natural Science Foundation of China for Youth Student Fundamental Research (Ph.D. Student)</span>
                            <br>
                            Research on Text-Guided Long Video Generation based on Spatiotemporal Native Collaboration 
                            <br>
                            May 2024 - December 2026
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <h2>Selected Paper Publications [<a href="https://scholar.google.com.hk/citations?user=BJvM6gsAAAAJ&hl=zh-CN&oi=sra">Google Scholar<a>]</h2>

            <br>
                <b>In the Year of 2026:</b>
            <br>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://arxiv.org/pdf/2511.08251"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> LayerEdit: Disentangled Multi-Object Editing via Conflict-Aware Multi-Layer Learning</span>
                            <br> Fengyi Fu, <b>Mengqi Huangüìß</b>, Lei Zhang, Zhendong Mao<br>
                            <b><font color="blue">AAAI 2026</font> </b>
                                <a href="https://github.com/fufy1024/LayerEdit"><img alt="Build" src="https://img.shields.io/github/stars/fufy1024/LayerEdit"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <br>
                <b>In the Year of 2025:</b>
            <br>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://pubmed.ncbi.nlm.nih.gov/41105542/"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> RealCustom++: Representing Images as Real Textual Word for Real-Time Customization</span>
                            <br> Zhendong Mao, <b>Mengqi Huang</b>, Fei Ding, Mingcong Liu, Qian He, Yongdong Zhang<br>
                            <b><font color="blue">IEEE Transactions on Pattern Analysis and Machine Intelligence</font> </b>
                                <a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://arxiv.org/pdf/2506.00512"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> Pro3D-Editor: A Progressive-Views Perspective for Consistent and Precise 3D Editing</span>
                            <br> Yang Zheng, <b>Mengqi Huangüìß</b>, Nan Chen, Zhendong Mao<br>
                            <b><font color="blue">NeurIPS 2025</font> </b>
                                <a href="https://github.com/shuoyueli4519/Pro3D-Editor-Code"><img alt="Build" src="https://img.shields.io/github/stars/shuoyueli4519/Pro3D-Editor-Code"></a> 
                                <a href="https://shuoyueli4519.github.io/Pro3D-Editor/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-Pro3D-yellow"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://arxiv.org/pdf/2504.02160"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</span>
                            <br> Shaojin Wu, <b>Mengqi Huangüìß</b>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He<br>
                            <b><font color="blue">ICCV 2025</font> </b>
                                <a href="https://github.com/bytedance/UNO"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/UNO"></a> 
                                <a href="https://bytedance.github.io/UNO/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://arxiv.org/pdf/2503.10406"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models</span>
                            <br> Yijing Lin, <b>Mengqi Huang</b>, Shuhan Zhuang, Zhendong Mao<br>
                            <b><font color="blue">ICCV 2025</font> </b>
                                <a href="https://github.com/Lyne1/Realgeneral"><img alt="Build" src="https://img.shields.io/github/stars/Lyne1/Realgeneral"></a> 
                                <a href="https://lyne1.github.io/realgeneral_web/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealGeneral-yellow"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://arxiv.org/pdf/2505.02192"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</span>
                            <br> Wenchuan Wang, <b>Mengqi Huang</b>, Yijing Tu, Zhendong Mao<br>
                            <b><font color="blue">ICCV 2025</font> </b>
                                <a href="https://github.com/wenc-k/DualReal"><img alt="Build" src="https://img.shields.io/github/stars/wenc-k/DualReal"></a> 
                                <a href="https://wenc-k.github.io/dualreal-customization/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-DualReal-yellow"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://arxiv.org/pdf/2507.01945"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> LongAnimation: Long Animation Generation with Dynamic Global-Local Memory</span>
                            <br> Nan Chen, <b>Mengqi Huang</b>, Yihao Meng, Zhendong Mao<br>
                            <b><font color="blue">ICCV 2025</font> </b>
                                <a href="https://github.com/CN-makers/LongAnimation"><img alt="Build" src="https://img.shields.io/github/stars/CN-makers/LongAnimation"></a> 
                                <a href="https://cn-makers.github.io/long_animation_web/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-LongAnimation-yellow"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Tu_A4A_Adapter_for_Adapter_Transfer_via_All-for-All_Mapping_for_Cross-Architecture_CVPR_2025_paper.pdf"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> A4A: Adapter for Adapter Transfer via All-for-All Mapping for Cross-Architecture Models</span>
                            <br> Keyu Tu, <b>Mengqi Huang</b>, Zhuowei Chen, Zhendong Mao<br>
                            <b><font color="blue">CVPR 2025</font> </b> <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Jia_D2iT_Dynamic_Diffusion_Transformer_for_Accurate_Image_Generation_CVPR_2025_paper.pdf"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation</span>
                            <br> Weinan Jia, <b>Mengqi Huang</b>, Nan Chen, Lei Zhang, Zhendong Mao<br>
                            <b><font color="blue">CVPR 2025</font> </b> <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Guang_Dragin3D_Image_Editing_by_Dragging_in_3D_Space_CVPR_2025_paper.pdf"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> Dragin3D: Image Editing by Dragging in 3D Space </span>
                            <br> Weiran Guang, Xiaoguang Gu, <b>Mengqi Huang</b>, Zhendong Mao<br>
                            <b><font color="blue">CVPR 2025</font> </b> <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_FeedEdit_Text-Based_Image_Editing_with_Dynamic_Feedback_Regulation_CVPR_2025_paper.pdf"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> FeedEdit: Text-Based Image Editing with Dynamic Feedback Regulation </span>
                            <br> Fengyi Fu, Lei Zhang, <b>Mengqi Huang</b>, Zhendong Mao<br>
                            <b><font color="blue">CVPR 2025</font> </b> <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32210/34365"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven Text-to-Image Customization</span>
                            <br> Nan Chen, <b>Mengqi Huang</b>, Zhuowei Chen, Yang Zheng, Lei Zhang, Zhendong Mao<br>
                            <b><font color="blue">AAAI 2025</font> </b> <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <br>
                <b>In the Year of 2024:</b>
            <br>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</span>
                            <br> 
                                <b>Mengqi Huang</b>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang
                            <br>
                                <b><font color="blue">CVPR 2024</font> </b>
                                <a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a> 
                                <a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a> 
                            <br>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28089"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing</span>
                            <br>
                                Hao Li, <b>Mengqi Huang</b>, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao
                            <br> 
                                <b><font color="blue">AAAI 2024</font> </b>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27891"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title"> DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation</span>
                            <br>
                                Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, <b>Mengqi Huang</b>, Yongdong Zhang, Zhendong Mao
                            <br> 
                                <b><font color="blue">AAAI 2024</font> </b>
                                <a href="https://dreamidentity.github.io/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-DreamIdentity-yellow"></a> 
                        </td>
                    </tr>
                </tbody>
            </table>

            <br>
                <b>In the Year of 2023 & 2022:</b>
            <br>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title">Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization 
                                <font color="red">(Highlight, 2.5% of submitted papers)</font> 
                            </span>
                            <br>
                                <b>Mengqi Huang</b>, Zhendong Mao, Zhuowei Chen, Yongdong Zhang
                            <br> 
                                <b><font color="blue">CVPR 2023</font> </b>
                                <a href="https://github.com/CrossmodalGroup/DynamicVectorQuantization"><img alt="Build" src="https://img.shields.io/github/stars/CrossmodalGroup/DynamicVectorQuantization"></a> 
                                <a href="https://youtu.be/ir60YW9JCjU"><img src="https://img.shields.io/badge/Video-DynamicVQ-red?logo=youtube"></a>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title">Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation</span>
                            <br>
                                <b>Mengqi Huang</b>, Zhendong Mao, Quan Wang, Yongdong Zhang
                            <br> 
                                <b><font color="blue">CVPR 2023</font> </b>
                                <a href="https://github.com/CrossmodalGroup/MaskedVectorQuantization"><img alt="Build" src="https://img.shields.io/github/stars/CrossmodalGroup/MaskedVectorQuantization"></a> 
                                <a href="https://www.youtube.com/watch?v=o2eyRscEejw"><img src="https://img.shields.io/badge/Video-MaskedVQ-red?logo=youtube"></a>
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <a href="https://arxiv.org/abs/2209.01339"><img src="./pics/pdf.gif"><br>PDF</a>
                        </td>
                        <td>
                            <span class="title">DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation 
                                <font color="red">(Best Student Paper Award, 1/3009 of submitted papers)</font> </span>
                            <br>
                                <b>Mengqi Huang</b>, Zhendong Mao, Penghui Wang, Quan Wang, Yongdong Zhang
                            <br>
                                <b><font color="blue">ACM Multimedia 2022</font> </b>
                            <br> 
                        </td>
                    </tr>
                </tbody>
            </table>

            <!-- <h2 style="margin-bottom: 16px;">Awards</h2>
            <ul style="
                list-style: disc inside;
                padding-left: 10px;
                font-size: 1.08em;
                line-height: 1.8;
                margin: 0;
                ">
              <li>‰∏≠ÂõΩÂõæË±°ÂõæÂΩ¢Â≠¶Â≠¶‰ºöËá™ÁÑ∂ÁßëÂ≠¶Â•ñ, ‰∏ÄÁ≠âÂ•ñ, 2025</li>
              <li>‰∏≠ÂõΩÁßëÂ≠¶Èô¢Èô¢ÈïøÁâπÂà´Â•ñ, 2025</li>
              <li>Best Student Paper Award, ACM Multimedia 2022 (First Author)</li>
            </ul> -->

            <h2 style="margin-bottom: 16px;">Awards</h2>
            <div class="awards-container">
                <div class="award-item">
                    <span class="award-title">‰∏≠ÂõΩÂõæË±°ÂõæÂΩ¢Â≠¶Â≠¶‰ºöËá™ÁÑ∂ÁßëÂ≠¶Â•ñ, ‰∏ÄÁ≠âÂ•ñ</span>
                    <span class="award-year">2025</span>
                </div>
                <div class="award-item">
                    <span class="award-title">‰∏≠ÂõΩÁßëÂ≠¶Èô¢Èô¢ÈïøÁâπÂà´Â•ñ</span>
                    <span class="award-year">2025</span>
                </div>
                <div class="award-item">
                    <span class="award-title">Best Student Paper Award, ACM Multimedia 2022 (First Author)</span>
                    <span class="award-year">2022</span>
                </div>
            </div>

            <!-- <h2 style="CLEAR: both">Internships</h2>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                          <img src="./pics/bytedance.jpeg" height="24"><br>
                        </td>
                        <td>
                          <span class="title"> <a href="https://bytedance.com/">ByteDance Inc.</a></span>
                          <br>Research Intern, Intelligent Creation Department.
                          <br>Beijing 
                          <br>July 2023 - Now

                        <br>
                          <div class="project-section">
                            <div class="project-card">
                              <div class="project-image">
                                <img src="./pics/teaser_realcustom.jpg" width="100%">
                              </div>
                              <div class="project-text">
                                <div class="project-title">
                                  RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization
                                </div>
                                <b><font color="blue">CVPR 2024 & IEEE T-PAMI</font> </b><br>
                                <div class="project-authors">
                                  <strong>Mengqi Huang</strong>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang
                                </div>
                                <div class="project-links">
                                    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/CVPR2024-RealCustom-b31b1b.svg"></a>
                                    <a href="https://pubmed.ncbi.nlm.nih.gov/41105542/"><img alt="Build" src="https://img.shields.io/badge/TPAMI-RealCustom++-b31b1b.svg"></a>
                                    <a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a> 
                                    <a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a> 
                                </div>
                                <div class="project-desc">
                                  We present RealCustom to disentangle subject similarity from text controllability and thereby allows both to be optimized simultaneously without conflicts. The core idea of RealCustom is to represent given subjects as real words that can be seamlessly integrated with given texts, and further leveraging the relevance between real words and image regions to disentangle visual condition from text condition.
                                </div>
                              </div>
                            </div>
                          </div>

                        <br>
                          <div class="project-section">
                            <div class="project-card">
                              <div class="project-image">
                                <img src="./pics/teaser_uno.jpg" width="100%">
                              </div>
                              <div class="project-text">
                                <div class="project-title">
                                    Less-to-More Generalization: Unlocking More Controllability by In-Context Generation
                                </div>
                                <b><font color="blue">ICCV 2025</font> </b><br>
                                <div class="project-authors">
                                    Shaojin Wu, <strong>Mengqi Huangüìß(Corresponding Author)</strong>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He
                                </div>
                                <div class="project-links">
                                    <a href="https://arxiv.org/abs/2504.02160"><img alt="Build" src="https://img.shields.io/badge/ICCV2025%20paper-UNO-b31b1b.svg"></a>
                                    <a href="https://github.com/bytedance/UNO"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/UNO"></a>
                                    <a href="https://bytedance.github.io/UNO/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a> 
                                </div>
                                <div class="project-desc">
                                    We propose a highly-consistent data synthesis pipeline for single-subject and multi-subject driven generation. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Moreover, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding.
                                </div>
                              </div>
                            </div>
                          </div>
                        </td>
                      </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <img src="./pics/kwai-logo.jpeg" height="24"><br>
                        </td>
                        <td>
                            <span class="title"> <a href="https://www.kuaishou.com/">Kuaishou Technology</a></span>
                            <br>Research Intern, Search Technology Department.
                            <br>Beijing 
                            <br>March 2022 - Novemeber 2022
                        </td>
                    </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                    <tr>
                        <td class="left">
                            <img src="./pics/mcmaster.jpeg" height="24"><br>
                        </td>
                        <td>
                            <span class="title"><a href="https://www.mcmaster.ca/">McMaster University</a></span>
                            <br>Research Intern, Computing & Software School (Department) 
                            <br>Hamilton, Canada
                            <br>June 2019 - September 2019
                        </td>
                    </tr>
                </tbody>
            </table> -->

            <h2 style="CLEAR: both">Internships</h2>

            <div class="internship-card">
                <div class="internship-header">
                    <img src="./pics/bytedance.jpeg" height="24">
                    <span class="title"> <a href="https://bytedance.com/">ByteDance Inc.</a></span>
                </div>
                <div class="internship-details">
                    Research Intern, Intelligent Creation Department.<br>
                    Beijing<br>
                    July 2023 ~ 
                </div>

                <div class="project-section">
                    <div class="project-card">
                        <div class="project-image">
                            <img src="./pics/teaser_realcustom.jpg" width="100%">
                        </div>
                        <div class="project-text">
                            <div class="project-title">
                                RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization
                            </div>
                            <b><font color="blue">CVPR 2024 & IEEE T-PAMI 2025</font></b><br>
                            <div class="project-authors">
                                <strong>Mengqi Huang</strong>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang
                            </div>
                            <div class="project-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/CVPR2024-RealCustom-b31b1b.svg"></a>
                                <a href="https://pubmed.ncbi.nlm.nih.gov/41105542/"><img alt="Build" src="https://img.shields.io/badge/TPAMI-RealCustom++-b31b1b.svg"></a>
                                <a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a>
                                <a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a>
                            </div>
                            <div class="project-desc">
                                We present RealCustom to disentangle subject similarity from text controllability and thereby allows both to be optimized simultaneously without conflicts. The core idea of RealCustom is to represent given subjects as real words that can be seamlessly integrated with given texts, and further leveraging the relevance between real words and image regions to disentangle visual condition from text condition.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="project-section">
                    <div class="project-card">
                        <div class="project-image">
                            <img src="./pics/teaser_uno.jpg" width="100%">
                        </div>
                        <div class="project-text">
                            <div class="project-title">
                                Less-to-More Generalization: Unlocking More Controllability by In-Context Generation
                            </div>
                            <b><font color="blue">ICCV 2025</font></b><br>
                            <div class="project-authors">
                                Shaojin Wu, <strong>Mengqi Huangüìß(Corresponding Author)</strong>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He
                            </div>
                            <div class="project-links">
                                <a href="https://arxiv.org/abs/2504.02160"><img alt="Build" src="https://img.shields.io/badge/ICCV2025%20paper-UNO-b31b1b.svg"></a>
                                <a href="https://github.com/bytedance/UNO"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/UNO"></a>
                                <a href="https://bytedance.github.io/UNO/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a>
                            </div>
                            <div class="project-desc">
                                We propose a highly-consistent data synthesis pipeline for single-subject and multi-subject driven generation. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Moreover, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding.
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <h2 style="CLEAR: both">Competitions</h2>

            <table>

            <h2 style="CLEAR: both">Competitions</h2>

            <table>
                <tbody>
                <tr>
                  <td>
                    <span class="title">
                        <a href="https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af50464a0ed647faca6266&award=1,000,000">
                            Á¨¨‰∫åÂ±äÁ≤§Ê∏ØÊæ≥Â§ßÊπæÂå∫ÂõΩÈôÖÁÆóÊ≥ïÁÆó‰æãÂ§ßËµõ-È´òÊïàÂèØÊéßÁöÑÊñáÁîüÂõæÊñπÊ≥ï
                        </a>
                    </span> <br>
                    Team Leader, Second Prize.
                    <br>
                    August 2023 - Novemeber 2023 <br>
                    <br>
                  </td>
                </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                <tr>
                  <td>
                    <span class="title">
                        <a href="http://www.aiinnovation.com.cn/#/">
                            È¶ñÂ±äÂÖ¥Êô∫ÊùØÂÖ®ÂõΩ‰∫∫Â∑•Êô∫ËÉΩÂ∫îÁî®ÂàõÊñ∞Â§ßËµõ-Â§öÊ®°ÊÄÅÊäÄÊúØÂàõÊñ∞Ëµõ-Âü∫‰∫éÊñáÊú¨ÁöÑÂõæÂÉèÁîüÊàê
                        </a>
                    </span> <br>
                    Team Leader, Second Prize.
                    <br>
                    August 2022 - Novemeber 2022 <br>
                    <br>
                  </td>
                </tr>
                </tbody>
            </table>

            <table>
                <tbody>
                <tr>
                  <td>
                    <span class="title">
                        <a href="http://smp-challenge.com/">ACM Multimedia 2020 Social Media Prediction Challenge</a>
                    </span> <br>
                    Team Leader, Top Performance Award. 
                    [<a href="https://github.com/Corleone-Huang/Social-Media-Popularity-Prediction-Challenge-2020">Github<a>]
                    <br>
                    March 2020 - June 2020 <br>
                    <br>
                  </td>
                </tr>
                </tbody>
            </table>

            <br>
            <a href="https://clustrmaps.com/site/1bpao" title="Visit tracker">
                <div>
                    <img src="http://clustrmaps.com/map_v2.png?d=x9zZiIk7-mY5aFcqqnSdkP6fOQeNgwkz4dAD8Esq9hE&cl=ffffff" height="100"/>
                </div>
            </a>

            <p>Last update: October, 2025. Webpage template borrows from <a href="http://wnzhang.net/">Weinan Zhang</a>.</p>
        </div>

        <div id="news">
            <h2>News</h2><br>

            <font size="3px">
                <b>November 2025</b><br>
                <span class="easylink">1 paper is accepted by AAAI 2026</span><br><br>
            </font>

            <font size="3px">
                <b>October 2025</b><br>
                <span class="easylink">1 paper is accepted by T-PAMI!!</span><br><br>
            </font>

            <font size="3px">
                <b>September 2025</b><br>
                <span class="easylink">1 paper is accepted by NeurIPS 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>June 2025</b><br>
                <span class="easylink">4 papers are accepted by ICCV 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>February 2025</b><br>
                <span class="easylink">4 papers are accepted by CVPR 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>December 2024</b><br>
                <span class="easylink">1 papers is accepted by AAAI 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>February 2024</b><br>
                <span class="easylink">1 papers is accepted by CVPR 2024!</span><br><br>
            </font>

            <font size="3px">
                <b>December 2023</b><br>
                <span class="easylink">2 papers are accepted by AAAI 2024!</span><br><br>
            </font>

            <font size="3px">
                <b>March 2023</b><br>
                <span class="easylink">Our paper "Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization" is selected as a <font color="red">highlight</font> at CVPR 2023!</span><br><br>
            </font>

            <font size="3px">
            <b>February 2023</b><br>
            <span class="easylink">2 papers are accepted by CVPR 2023!</span><br><br>
            </font>

            <font size="3px">
            <b>October 2022</b><br>
            <span class="easylink">Our paper "DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation" receives the <font color="red">Best Student Paper Award</font> at ACM Multimedia 2022!</span><br><br>
            </font>

            <font size="3px">
            <b>June 2022</b><br>
            <span class="easylink">1 paper is accepted by ACM Multimedia 2022!</span><br><br>
            </font>
        </div>
    </div>
</body>
</html>