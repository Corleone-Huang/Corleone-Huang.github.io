<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
    <meta content="IE=7.0000" http-equiv="X-UA-Compatible">
    <title>Mengqi Huang - USTC</title>
    <meta name="description" content="Mengqi Huang, Ph.D. , University of Science and Technology of China. ">
    <meta name="keywords" content="mengqi huang, ustc, university of science and technoloy of china, deep learning, image generation, cross-modality learning">
    <meta content="text/html;charset=utf-8" http-equiv="Content-Type">
    <link rel="stylesheet" type="text/css" href="template.css" />
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-39532305-5', 'auto');
        ga('send', 'pageview');
    </script>
</head>
<link rel="shortcut icon" href="./pics/ustc-favicon.png"/>

<body>
    <div id="header">
        <img style="margin-bottom:10pt;margin-top:10pt;margin-left:10%;" src="./pics/ustc-logo.png" height="67">
    </div>

    <div id="content">
        <div id="left">
            <table style="background-color:white;">
                <tr NOSAVE>
                    <td valign="CENTER">
                        <img src="pics/self.jpg" height="260" align="center">
                    </td>

                    <td valign="CENTER" width="2%"></td>

                    <td valign="CENTER" halign="LEFT">
                        <font size="+0">
                            <b><font size="+2">Mengqi Huang&nbsp;</font></b>
                            <p style="margin-left:0px;">
                                <img src="./pics/huangmq_name.png" height="64">
                            <p style="margin-left:0px;">
                            <!-- <b>Ph.D. </b> -->
                            <p style="margin-left:0px;">
                                University of Science and Technology of China<br>
                            <!-- <p style="margin-left:0px;">
                                443 Huangshan Road, Hefei, China 230027<br> -->
                            <p style="margin-left:0px;">
                                Email: huangmq@ustc.edu.cn<br>
                            <p>
                        </font>
                    </td>
                </tr>
            </table>

            <div style="margin-top:20px;">
                <p>
                <br> I received my Ph.D. degree from the University of Science and Technology of China (USTC) in 2025. My research interests include deep generative models, image/video generation, and unified multimodal models. I am the recipient of the Best Student Paper Award at ACM Multimedia 2022 as the first author. I have also received funding from the First National Natural Science Foundation of China for Youth Student Fundamental Research (Ph.D. student).
                <br> 
                <br> 2025å¹´äºä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦è·åšå£«å­¦ä½ã€‚åšå£«æœŸé—´ä»¥ç¬¬ä¸€ä½œè€…è·CCF-Aç±»å›½é™…ä¼šè®®ACM Multimedia 2022æœ€ä½³å­¦ç”Ÿè®ºæ–‡å¥–ã€‚è·æ‰¹é¦–å±Šå›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é’å¹´å­¦ç”ŸåŸºç¡€ç ”ç©¶é¡¹ç›®(åšå£«ç ”ç©¶ç”Ÿ)ï¼Œå…¥é€‰é¦–å±Šä¸­å›½ç§‘åé’å¹´äººæ‰æ‰˜ä¸¾å·¥ç¨‹åšå£«ç”Ÿä¸“é¡¹(ä¸­å›½ç”µå­å­¦ä¼šæ‰˜ä¸¾)ã€‚è·2025å¹´ä¸­å›½ç§‘å­¦é™¢é™¢é•¿ç‰¹åˆ«å¥–ã€‚æ¯•ä¸šæ›¾å…¥é€‰é˜¿é‡Œæ˜Ÿ(2025 AliStar Program - P7)ã€å­—èŠ‚è·³åŠ¨å¤´éƒ¨äººæ‰è®¡åˆ’ç­‰ã€‚
            </div> 

            <h2>Education</h2>
            <div class="education-container">
                <div class="education-card">
                    <div class="education-logo">
                        <img src="pics/ustc-logo-single.png" alt="ustc" width="60%">
                    </div>
                    <div class="education-details">
                        <p>
                            <a href="http://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a><br>
                            Ph.D. , September 2020 ~ <br>
                            Supervisor: <a href="https://scholar.google.com.hk/citations?user=m-0P8sgAAAAJ&hl=zh-CN">Prof. Zhendong Mao</a>
                        </p>
                    </div>
                </div>
                <div class="education-card">
                    <div class="education-logo">
                        <img src="pics/ustc-logo-single.png" alt="ustc" width="60%">
                    </div>
                    <div class="education-details">
                        <p>
                            <a href="http://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a><br>
                            B.Eng. , August 2016 ~ June 2020<br>
                        </p>
                    </div>
                </div>
            </div>

            <h2>Funding</h2>
            <div class="funding-container">
                <div class="funding-card">
                    <div class="funding-logo">
                        <img src="pics/National_Natural_Science_Foundation_of_China_logo.png" alt="nsfc" width="60%">
                    </div>
                    <div class="funding-details">
                        <div class="funding-source">National Natural Science Foundation of China (NSFC)</div>
                        <div class="funding-title">Research on Text-Guided Long Video Generation based on Spatiotemporal Native Collaboration</div>
                        <div class="funding-role">PI.</div>
                        <div class="funding-duration">2024.05 ~ 2026.12</div>
                    </div>
                </div>
            </div>

            <h2>Selected Paper Publications [<a href="https://scholar.google.com.hk/citations?user=BJvM6gsAAAAJ&hl=zh-CN&oi=sra">Google Scholar<a>]</h2>

            <!-- <br>
                <b>In the Year of 2026:</b>
            <br> -->
            <div class="year-heading">In the Year of <span class="year-value">2026</span></div>

            <!-- LayerEdit -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">LayerEdit: Disentangled Multi-Object Editing via Conflict-Aware Multi-Layer Learning</span>
                    <div class="paper-authors">Fengyi Fu, <b>Mengqi HuangğŸ“§</b>, Lei Zhang, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">AAAI 2026</font></b>
                        <a href="https://arxiv.org/pdf/2511.08251"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/fufy1024/LayerEdit"><img alt="Build" src="https://img.shields.io/github/stars/fufy1024/LayerEdit"></a>
                    </div>
                </div>
            </div>

            <!-- <br>
                <b>In the Year of 2025:</b>
            <br> -->
            <div class="year-heading">In the Year of <span class="year-value">2025</span></div>

            <!-- RealCustom++ (TPAMI) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">RealCustom++: Representing Images as Real Textual Word for Real-Time Customization</span>
                    <div class="paper-authors">Zhendong Mao, <b>Mengqi Huang</b>, Fei Ding, Mingcong Liu, Qian He, Yongdong Zhang</div>
                    <div class="paper-badges">
                        <b><font color="blue">IEEE Transactions on Pattern Analysis and Machine Intelligence</font></b>
                        <a href="https://ieeexplore.ieee.org/abstract/document/11206511"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a>
                    </div>
                </div>
            </div>

            <!-- Pro3D-Editor (NeurIPS 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">Pro3D-Editor: A Progressive-Views Perspective for Consistent and Precise 3D Editing</span>
                    <div class="paper-authors">Yang Zheng, <b>Mengqi HuangğŸ“§</b>, Nan Chen, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">NeurIPS 2025</font></b>
                        <a href="https://arxiv.org/pdf/2506.00512"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/shuoyueli4519/Pro3D-Editor-Code"><img alt="Build" src="https://img.shields.io/github/stars/shuoyueli4519/Pro3D-Editor-Code"></a>
                        <a href="https://shuoyueli4519.github.io/Pro3D-Editor/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-Pro3D-yellow"></a>
                    </div>
                </div>
            </div>

            <!-- UNO (ICCV 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</span>
                    <div class="paper-authors">Shaojin Wu, <b>Mengqi HuangğŸ“§</b>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He</div>
                    <div class="paper-badges">
                        <b><font color="blue">ICCV 2025</font></b>
                        <a href="https://arxiv.org/pdf/2504.02160"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/bytedance/UNO"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/UNO"></a>
                        <a href="https://bytedance.github.io/UNO/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a>
                    </div>
                </div>
            </div>

            <!-- RealGeneral (ICCV 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models</span>
                    <div class="paper-authors">Yijing Lin, <b>Mengqi Huang</b>, Shuhan Zhuang, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">ICCV 2025</font></b>
                        <a href="https://arxiv.org/pdf/2503.10406"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/Lyne1/Realgeneral"><img alt="Build" src="https://img.shields.io/github/stars/Lyne1/Realgeneral"></a>
                        <a href="https://lyne1.github.io/realgeneral_web/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealGeneral-yellow"></a>
                    </div>
                </div>
            </div>

            <!-- DualReal (ICCV 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</span>
                    <div class="paper-authors">Wenchuan Wang, <b>Mengqi Huang</b>, Yijing Tu, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">ICCV 2025</font></b>
                        <a href="https://arxiv.org/pdf/2505.02192"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/wenc-k/DualReal"><img alt="Build" src="https://img.shields.io/github/stars/wenc-k/DualReal"></a>
                        <a href="https://wenc-k.github.io/dualreal-customization/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-DualReal-yellow"></a>
                    </div>
                </div>
            </div>

            <!-- LongAnimation (ICCV 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">LongAnimation: Long Animation Generation with Dynamic Global-Local Memory</span>
                    <div class="paper-authors">Nan Chen, <b>Mengqi Huang</b>, Yihao Meng, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">ICCV 2025</font></b>
                        <a href="https://arxiv.org/pdf/2507.01945"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/CN-makers/LongAnimation"><img alt="Build" src="https://img.shields.io/github/stars/CN-makers/LongAnimation"></a>
                        <a href="https://cn-makers.github.io/long_animation_web/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-LongAnimation-yellow"></a>
                    </div>
                </div>
            </div>

            <!-- A4A (CVPR 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">A4A: Adapter for Adapter Transfer via All-for-All Mapping for Cross-Architecture Models</span>
                    <div class="paper-authors">Keyu Tu, <b>Mengqi Huang</b>, Zhuowei Chen, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">CVPR 2025</font></b>
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Tu_A4A_Adapter_for_Adapter_Transfer_via_All-for-All_Mapping_for_Cross-Architecture_CVPR_2025_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                    </div>
                </div>
            </div>

            <!-- D^2iT (CVPR 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation</span>
                    <div class="paper-authors">Weinan Jia, <b>Mengqi Huang</b>, Nan Chen, Lei Zhang, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">CVPR 2025</font></b>
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Jia_D2iT_Dynamic_Diffusion_Transformer_for_Accurate_Image_Generation_CVPR_2025_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                    </div>
                </div>
            </div>

            <!-- Dragin3D (CVPR 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">Dragin3D: Image Editing by Dragging in 3D Space</span>
                    <div class="paper-authors">Weiran Guang, Xiaoguang Gu, <b>Mengqi Huang</b>, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">CVPR 2025</font></b>
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Guang_Dragin3D_Image_Editing_by_Dragging_in_3D_Space_CVPR_2025_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                    </div>
                </div>
            </div>

            <!-- FeedEdit (CVPR 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">FeedEdit: Text-Based Image Editing with Dynamic Feedback Regulation</span>
                    <div class="paper-authors">Fengyi Fu, Lei Zhang, <b>Mengqi Huang</b>, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">CVPR 2025</font></b>
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_FeedEdit_Text-Based_Image_Editing_with_Dynamic_Feedback_Regulation_CVPR_2025_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                    </div>
                </div>
            </div>

            <!-- CustomContrast (AAAI 2025) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven Text-to-Image Customization</span>
                    <div class="paper-authors">Nan Chen, <b>Mengqi Huang</b>, Zhuowei Chen, Yang Zheng, Lei Zhang, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">AAAI 2025</font></b>
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32210/34365"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                    </div>
                </div>
            </div>

            <!-- <br>
                <b>In the Year of 2024:</b>
            <br> -->
            <div class="year-heading">In the Year of <span class="year-value">2024</span></div>

            <!-- RealCustom (CVPR 2024) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</span>
                    <div class="paper-authors"><b>Mengqi Huang</b>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang</div>
                    <div class="paper-badges">
                        <b><font color="blue">CVPR 2024</font></b>
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a>
                        <a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a>
                    </div>
                </div>
            </div>

            <!-- Gradual Residuals Alignment (AAAI 2024) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing</span>
                    <div class="paper-authors">Hao Li, <b>Mengqi Huang</b>, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">AAAI 2024</font></b>
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28089"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                    </div>
                </div>
            </div>

            <!-- DreamIdentity (AAAI 2024) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation</span>
                    <div class="paper-authors">Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, <b>Mengqi Huang</b>, Yongdong Zhang, Zhendong Mao</div>
                    <div class="paper-badges">
                        <b><font color="blue">AAAI 2024</font></b>
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27891"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://dreamidentity.github.io/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-DreamIdentity-yellow"></a>
                    </div>
                </div>
            </div>

            <!-- <br>
                <b>In the Year of 2023 & 2022:</b>
            <br> -->
            <div class="year-heading">In the Year of <span class="year-value">2023 &amp; 2022</span></div>

            <!-- CVPR 2023 DynamicVQ (Highlight) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization <font color="red">(Highlight, 2.5% of submitted papers)</font></span>
                    <div class="paper-authors"><b>Mengqi Huang</b>, Zhendong Mao, Zhuowei Chen, Yongdong Zhang</div>
                    <div class="paper-badges">
                        <b><font color="blue">CVPR 2023</font></b>
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/CrossmodalGroup/DynamicVectorQuantization"><img alt="Build" src="https://img.shields.io/github/stars/CrossmodalGroup/DynamicVectorQuantization"></a>
                        <a href="https://youtu.be/ir60YW9JCjU"><img src="https://img.shields.io/badge/Video-DynamicVQ-red?logo=youtube"></a>
                    </div>
                </div>
            </div>

            <!-- CVPR 2023 MaskedVQ -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation</span>
                    <div class="paper-authors"><b>Mengqi Huang</b>, Zhendong Mao, Quan Wang, Yongdong Zhang</div>
                    <div class="paper-badges">
                        <b><font color="blue">CVPR 2023</font></b>
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                        <a href="https://github.com/CrossmodalGroup/MaskedVectorQuantization"><img alt="Build" src="https://img.shields.io/github/stars/CrossmodalGroup/MaskedVectorQuantization"></a>
                        <a href="https://www.youtube.com/watch?v=o2eyRscEejw"><img src="https://img.shields.io/badge/Video-MaskedVQ-red?logo=youtube"></a>
                    </div>
                </div>
            </div>

            <!-- ACM MM 2022 DSE-GAN (Best Student Paper Award) -->
            <div class="paper-card">
                <div class="paper-content">
                    <span class="paper-title">DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation <font color="red">(Best Student Paper Award, 1/3009 of submitted papers)</font></span>
                    <div class="paper-authors"><b>Mengqi Huang</b>, Zhendong Mao, Penghui Wang, Quan Wang, Yongdong Zhang</div>
                    <div class="paper-badges">
                        <b><font color="blue">ACM Multimedia 2022</font></b>
                        <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547881"><img alt="Build" src="https://img.shields.io/badge/Paper-Link-b31b1b.svg"></a>
                    </div>
                </div>
            </div>

            <h2>Awards</h2>
            <div class="awards-container">
                <div class="award-item">
                    <span class="award-title">ä¸­å›½å›¾è±¡å›¾å½¢å­¦å­¦ä¼šè‡ªç„¶ç§‘å­¦å¥–, ä¸€ç­‰å¥–</span>
                    <span class="award-year">2025</span>
                </div>
                <div class="award-item">
                    <span class="award-title">ä¸­å›½ç§‘å­¦é™¢é™¢é•¿ç‰¹åˆ«å¥–</span>
                    <span class="award-year">2025</span>
                </div>
                <div class="award-item">
                    <span class="award-title">Best Student Paper Award, ACM Multimedia 2022 (First Author)</span>
                    <span class="award-year">2022</span>
                </div>
            </div>

            <h2>Internships</h2>

            <div class="internship-card">
                <div class="internship-header">
                    <img src="./pics/bytedance.jpeg" height="40">
                    <span class="title"> <a href="https://bytedance.com/">ByteDance Inc.</a></span>
                </div>
                <div class="internship-details">
                    Research Intern, Intelligent Creation Department.<br>
                    Beijing<br>
                    July 2023 ~ 
                </div>

                <div class="project-section">
                    <div class="project-card">
                        <div class="project-image">
                            <img src="./pics/teaser_realcustom.jpg" width="100%">
                        </div>
                        <div class="project-text">
                            <div class="project-title">
                                RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization
                            </div>
                            <b><font color="blue">CVPR 2024 & IEEE T-PAMI 2025</font></b><br>
                            <div class="project-authors">
                                <strong>Mengqi Huang</strong>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang
                            </div>
                            <div class="project-links">
                                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf"><img alt="Build" src="https://img.shields.io/badge/CVPR2024-RealCustom-b31b1b.svg"></a>
                                <a href="https://pubmed.ncbi.nlm.nih.gov/41105542/"><img alt="Build" src="https://img.shields.io/badge/TPAMI-RealCustom++-b31b1b.svg"></a>
                                <a href="https://github.com/bytedance/RealCustom"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a>
                                <a href="https://corleone-huang.github.io/RealCustom_plus_plus/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a>
                            </div>
                            <div class="project-desc">
                                We present RealCustom to disentangle subject similarity from text controllability and thereby allows both to be optimized simultaneously without conflicts. The core idea of RealCustom is to represent given subjects as real words that can be seamlessly integrated with given texts, and further leveraging the relevance between real words and image regions to disentangle visual condition from text condition.
                            </div>
                        </div>
                    </div>
                </div>

                <div class="project-section">
                    <div class="project-card">
                        <div class="project-image">
                            <img src="./pics/teaser_uno.jpg" width="100%">
                        </div>
                        <div class="project-text">
                            <div class="project-title">
                                Less-to-More Generalization: Unlocking More Controllability by In-Context Generation
                            </div>
                            <b><font color="blue">ICCV 2025</font></b><br>
                            <div class="project-authors">
                                Shaojin Wu, <strong>Mengqi HuangğŸ“§(Corresponding Author)</strong>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He
                            </div>
                            <div class="project-links">
                                <a href="https://arxiv.org/abs/2504.02160"><img alt="Build" src="https://img.shields.io/badge/ICCV2025%20paper-UNO-b31b1b.svg"></a>
                                <a href="https://github.com/bytedance/UNO"><img alt="Build" src="https://img.shields.io/github/stars/bytedance/UNO"></a>
                                <a href="https://bytedance.github.io/UNO/"><img alt="Build" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a>
                            </div>
                            <div class="project-desc">
                                We propose a highly-consistent data synthesis pipeline for single-subject and multi-subject driven generation. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Moreover, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding.
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <h2>Competitions</h2>

            <div class="competitions-container">
                <div class="competition-item">
                    <div class="competition-title">
                        <a href="https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af50464a0ed647faca6266&award=1,000,000">ç¬¬äºŒå±Šç²¤æ¸¯æ¾³å¤§æ¹¾åŒºå›½é™…ç®—æ³•ç®—ä¾‹å¤§èµ›-é«˜æ•ˆå¯æ§çš„æ–‡ç”Ÿå›¾æ–¹æ³•</a>
                    </div>
                    <div class="competition-details">
                        Team Leader, Second Prize.
                    </div>
                    <div class="competition-date">
                        August 2023 - November 2023
                    </div>
                </div>

                <div class="competition-item">
                    <div class="competition-title">
                        <a href="http://www.aiinnovation.com.cn/#/">é¦–å±Šå…´æ™ºæ¯å…¨å›½äººå·¥æ™ºèƒ½åº”ç”¨åˆ›æ–°å¤§èµ›-å¤šæ¨¡æ€æŠ€æœ¯åˆ›æ–°èµ›-åŸºäºæ–‡æœ¬çš„å›¾åƒç”Ÿæˆ</a>
                    </div>
                    <div class="competition-details">
                        Team Leader, Second Prize.
                    </div>
                    <div class="competition-date">
                        August 2022 - November 2022
                    </div>
                </div>

                <div class="competition-item">
                    <div class="competition-title">
                        <a href="http://smp-challenge.com/">ACM Multimedia 2020 Social Media Prediction Challenge</a>
                    </div>
                    <div class="competition-details">
                        Team Leader, Top Performance Award. 
                        [<a href="https://github.com/Corleone-Huang/Social-Media-Popularity-Prediction-Challenge-2020">Github</a>]
                    </div>
                    <div class="competition-date">
                        March 2020 - June 2020
                    </div>
                </div>
            </div>

            <br>
            <a href="https://clustrmaps.com/site/1bpao" title="Visit tracker">
                <div>
                    <img src="http://clustrmaps.com/map_v2.png?d=x9zZiIk7-mY5aFcqqnSdkP6fOQeNgwkz4dAD8Esq9hE&cl=ffffff" height="100"/>
                </div>
            </a>

            <!-- <p>Last update: October, 2025. Webpage template borrows from <a href="http://wnzhang.net/">Weinan Zhang</a>.</p> -->
            <p>Last update: October, 2025.</a></p>
        </div>

        <div id="news">
            <h2>News</h2><br>

            <font size="3px">
                <b>November 2025</b><br>
                <span class="easylink">1 paper is accepted by AAAI 2026</span><br><br>
            </font>

            <font size="3px">
                <b>October 2025</b><br>
                <span class="easylink">1 paper is accepted by T-PAMI!!</span><br><br>
            </font>

            <font size="3px">
                <b>September 2025</b><br>
                <span class="easylink">1 paper is accepted by NeurIPS 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>June 2025</b><br>
                <span class="easylink">4 papers are accepted by ICCV 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>February 2025</b><br>
                <span class="easylink">4 papers are accepted by CVPR 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>December 2024</b><br>
                <span class="easylink">1 papers is accepted by AAAI 2025!</span><br><br>
            </font>

            <font size="3px">
                <b>February 2024</b><br>
                <span class="easylink">1 papers is accepted by CVPR 2024!</span><br><br>
            </font>

            <font size="3px">
                <b>December 2023</b><br>
                <span class="easylink">2 papers are accepted by AAAI 2024!</span><br><br>
            </font>

            <font size="3px">
                <b>March 2023</b><br>
                <span class="easylink">Our paper "Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization" is selected as a <font color="red">highlight</font> at CVPR 2023!</span><br><br>
            </font>

            <font size="3px">
            <b>February 2023</b><br>
            <span class="easylink">2 papers are accepted by CVPR 2023!</span><br><br>
            </font>

            <font size="3px">
            <b>October 2022</b><br>
            <span class="easylink">Our paper "DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation" receives the <font color="red">Best Student Paper Award</font> at ACM Multimedia 2022!</span><br><br>
            </font>

            <font size="3px">
            <b>June 2022</b><br>
            <span class="easylink">1 paper is accepted by ACM Multimedia 2022!</span><br><br>
            </font>
        </div>
    </div>
</body>
</html>