<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Mengqi Huang - USTC</title>
    <meta name="description" content="Mengqi Huang, Ph.D., University of Science and Technology of China.">
    <meta name="keywords" content="mengqi huang, ustc, university of science and technoloy of china, deep learning, image generation, cross-modality learning">
    <link rel="stylesheet" href="template.css">
    <link rel="shortcut icon" href="./pics/ustc-favicon.png">
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-39532305-5', 'auto');
        ga('send', 'pageview');
    </script>
</head>
<body>
    <header id="header">
        <img src="./pics/ustc-logo.png" height="67" alt="USTC Logo" style="margin: 10pt 10%;">
    </header>

    <div id="content">
        <main id="left">
            <section class="profile">
                <table style="background-color: white;">
                    <tr>
                        <td class="profile-image">
                            <img src="pics/self.jpg" height="260" alt="Mengqi Huang" class="center-image">
                        </td>
                        <td class="profile-spacer"></td>
                        <td class="profile-info">
                            <h1 class="name">Mengqi Huang</h1>
                            <img src="./pics/huangmq_name.png" height="64" alt="ÈªÑÊ¢¶Áê™">
                            <p class="degree"><strong>Ph.D.</strong></p>
                            <p class="institution">University of Science and Technology of China</p>
                            <p class="address">443 Huangshan Road, Hefei, China 230027</p>
                            <p class="email">Email: huangmq@mail.ustc.edu.cn</p>
                        </td>
                    </tr>
                </table>
            </section>

            <section class="introduction">
                <p>I received my Ph.D. degree from the University of Science and Technology of China (USTC) in 2025. My research interests include deep generative models, image/video generation, and unified multimodal generation. I am the recipient of the Best Student Paper Award at ACM Multimedia 2022 as the first author. I have also received funding from the First National Natural Science Foundation of China for Youth Student Fundamental Research (Ph.D. student).</p>
                <p>2025Âπ¥‰∫é‰∏≠ÂõΩÁßëÂ≠¶ÊäÄÊúØÂ§ßÂ≠¶Ëé∑ÂçöÂ£´Â≠¶‰Ωç„ÄÇÂçöÂ£´ÊúüÈó¥‰ª•Á¨¨‰∏Ä‰ΩúËÄÖËé∑CCF-AÁ±ªÂõΩÈôÖ‰ºöËÆÆACM Multimedia 2022ÊúÄ‰Ω≥Â≠¶ÁîüËÆ∫ÊñáÂ•ñ„ÄÇËé∑ÊâπÈ¶ñÂ±äÂõΩÂÆ∂Ëá™ÁÑ∂ÁßëÂ≠¶Âü∫ÈáëÈùíÂπ¥Â≠¶ÁîüÂü∫Á°ÄÁ†îÁ©∂È°πÁõÆ(ÂçöÂ£´Á†îÁ©∂Áîü)ÔºåÂÖ•ÈÄâÈ¶ñÂ±ä‰∏≠ÂõΩÁßëÂçèÈùíÂπ¥‰∫∫ÊâçÊâò‰∏æÂ∑•Á®ãÂçöÂ£´Áîü‰∏ìÈ°π(‰∏≠ÂõΩÁîµÂ≠êÂ≠¶‰ºöÊâò‰∏æ)„ÄÇËé∑2025Âπ¥‰∏≠ÂõΩÁßëÂ≠¶Èô¢Èô¢ÈïøÁâπÂà´Â•ñ„ÄÇÊØï‰∏öÊõæÂÖ•ÈÄâÈòøÈáåÊòü(the 2025 Alistar Program - P7)„ÄÅÂ≠óËäÇË∑≥Âä®Â§¥ÈÉ®‰∫∫ÊâçËÆ°ÂàíÁ≠â„ÄÇ</p>
            </section>

            <section class="education">
                <h2>Education</h2>
                <div class="timeline-item">
                    <div class="timeline-logo">
                        <img src="./pics/ustc-logo-single.png" height="24" alt="USTC Logo">
                    </div>
                    <div class="timeline-content">
                        <h3 class="timeline-title">University of Science and Technology of China</h3>
                        <p class="timeline-degree">Ph.D.</p>
                        <p class="timeline-period">September 2020 - June 2025</p>
                        <p class="timeline-advisor">Advisor: <a href="https://scholar.google.com.hk/citations?user=m-0P8sgAAAAJ&hl=zh-CN&oi=ao" target="_blank">Prof. Zhendong Mao</a></p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-logo">
                        <img src="./pics/ustc-logo-single.png" height="24" alt="USTC Logo">
                    </div>
                    <div class="timeline-content">
                        <h3 class="timeline-title">University of Science and Technology of China</h3>
                        <p class="timeline-degree">B.Eng</p>
                        <p class="timeline-period">September 2016 - June 2020</p>
                        <p class="timeline-advisor">Advisor: <a href="https://scholar.google.com.hk/citations?user=m-0P8sgAAAAJ&hl=zh-CN&oi=ao" target="_blank">Prof. Zhendong Mao</a></p>
                    </div>
                </div>
            </section>

            <section class="funding">
                <h2>Funding</h2>
                <div class="timeline-item">
                    <div class="timeline-logo">
                        <img src="./pics/National_Natural_Science_Foundation_of_China_logo.png" height="24" alt="NSFC Logo">
                    </div>
                    <div class="timeline-content">
                        <h3 class="timeline-title">The National Natural Science Foundation of China for Youth Student Fundamental Research (Ph.D. Student)</h3>
                        <p class="timeline-description">Research on Text-Guided Long Video Generation based on Spatiotemporal Native Collaboration</p>
                        <p class="timeline-period">May 2024 - December 2026</p>
                    </div>
                </div>
            </section>

            <section class="publications">
                <h2>Selected Paper Publications <a href="https://scholar.google.com.hk/citations?user=BJvM6gsAAAAJ&hl=zh-CN&oi=sra" class="scholar-link" target="_blank">[Google Scholar]</a></h2>

                <h3 class="year-heading">In the Year of 2026:</h3>
                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://arxiv.org/pdf/2511.08251" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">LayerEdit: Disentangled Multi-Object Editing via Conflict-Aware Multi-Layer Learning</h4>
                        <p class="paper-authors">Fengyi Fu, <strong>Mengqi Huangüìß</strong>, Lei Zhang, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">AAAI 2026</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/fufy1024/LayerEdit" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/fufy1024/LayerEdit"></a>
                        </div>
                    </div>
                </div>

                <h3 class="year-heading">In the Year of 2025:</h3>
                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://pubmed.ncbi.nlm.nih.gov/41105542/" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">RealCustom++: Representing Images as Real Textual Word for Real-Time Customization</h4>
                        <p class="paper-authors">Zhendong Mao, <strong>Mengqi Huang</strong>, Fei Ding, Mingcong Liu, Qian He, Yongdong Zhang</p>
                        <p class="paper-conference"><strong><font color="blue">IEEE Transactions on Pattern Analysis and Machine Intelligence</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/bytedance/RealCustom" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://arxiv.org/pdf/2506.00512" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">Pro3D-Editor: A Progressive-Views Perspective for Consistent and Precise 3D Editing</h4>
                        <p class="paper-authors">Yang Zheng, <strong>Mengqi Huangüìß</strong>, Nan Chen, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">NeurIPS 2025</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/shuoyueli4519/Pro3D-Editor-Code" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/shuoyueli4519/Pro3D-Editor-Code"></a>
                            <a href="https://shuoyueli4519.github.io/Pro3D-Editor/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-Pro3D-yellow"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://arxiv.org/pdf/2504.02160" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</h4>
                        <p class="paper-authors">Shaojin Wu, <strong>Mengqi Huangüìß</strong>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He</p>
                        <p class="paper-conference"><strong><font color="blue">ICCV 2025</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/bytedance/UNO" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/bytedance/UNO"></a>
                            <a href="https://bytedance.github.io/UNO/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://arxiv.org/pdf/2503.10406" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">RealGeneral: Unifying Visual Generation via Temporal In-Context Learning with Video Models</h4>
                        <p class="paper-authors">Yijing Lin, <strong>Mengqi Huang</strong>, Shuhan Zhuang, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">ICCV 2025</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/Lyne1/Realgeneral" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/Lyne1/Realgeneral"></a>
                            <a href="https://lyne1.github.io/realgeneral_web/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-RealGeneral-yellow"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://arxiv.org/pdf/2505.02192" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</h4>
                        <p class="paper-authors">Wenchuan Wang, <strong>Mengqi Huang</strong>, Yijing Tu, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">ICCV 2025</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/wenc-k/DualReal" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/wenc-k/DualReal"></a>
                            <a href="https://wenc-k.github.io/dualreal-customization/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-DualReal-yellow"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://arxiv.org/pdf/2507.01945" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">LongAnimation: Long Animation Generation with Dynamic Global-Local Memory</h4>
                        <p class="paper-authors">Nan Chen, <strong>Mengqi Huang</strong>, Yihao Meng, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">ICCV 2025</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/CN-makers/LongAnimation" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/CN-makers/LongAnimation"></a>
                            <a href="https://cn-makers.github.io/long_animation_web/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-LongAnimation-yellow"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Tu_A4A_Adapter_for_Adapter_Transfer_via_All-for-All_Mapping_for_Cross-Architecture_CVPR_2025_paper.pdf" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">A4A: Adapter for Adapter Transfer via All-for-All Mapping for Cross-Architecture Models</h4>
                        <p class="paper-authors">Keyu Tu, <strong>Mengqi Huang</strong>, Zhuowei Chen, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">CVPR 2025</font></strong></p>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Jia_D2iT_Dynamic_Diffusion_Transformer_for_Accurate_Image_Generation_CVPR_2025_paper.pdf" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation</h4>
                        <p class="paper-authors">Weinan Jia, <strong>Mengqi Huang</strong>, Nan Chen, Lei Zhang, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">CVPR 2025</font></strong></p>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Guang_Dragin3D_Image_Editing_by_Dragging_in_3D_Space_CVPR_2025_paper.pdf" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">Dragin3D: Image Editing by Dragging in 3D Space</h4>
                        <p class="paper-authors">Weiran Guang, Xiaoguang Gu, <strong>Mengqi Huang</strong>, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">CVPR 2025</font></strong></p>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Fu_FeedEdit_Text-Based_Image_Editing_with_Dynamic_Feedback_Regulation_CVPR_2025_paper.pdf" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">FeedEdit: Text-Based Image Editing with Dynamic Feedback Regulation</h4>
                        <p class="paper-authors">Fengyi Fu, Lei Zhang, <strong>Mengqi Huang</strong>, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">CVPR 2025</font></strong></p>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32210/34365" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven Text-to-Image Customization</h4>
                        <p class="paper-authors">Nan Chen, <strong>Mengqi Huang</strong>, Zhuowei Chen, Yang Zheng, Lei Zhang, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">AAAI 2025</font></strong></p>
                    </div>
                </div>

                <h3 class="year-heading">In the Year of 2024:</h3>
                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</h4>
                        <p class="paper-authors"><strong>Mengqi Huang</strong>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang</p>
                        <p class="paper-conference"><strong><font color="blue">CVPR 2024</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/bytedance/RealCustom" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a>
                            <a href="https://corleone-huang.github.io/RealCustom_plus_plus/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28089" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">Gradual Residuals Alignment: A Dual-Stream Framework for GAN Inversion and Image Attribute Editing</h4>
                        <p class="paper-authors">Hao Li, <strong>Mengqi Huang</strong>, Lei Zhang, Bo Hu, Yi Liu, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">AAAI 2024</font></strong></p>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27891" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">DreamIdentity: Improved Editability for Efficient Face-identity Preserved Image Generation</h4>
                        <p class="paper-authors">Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, <strong>Mengqi Huang</strong>, Yongdong Zhang, Zhendong Mao</p>
                        <p class="paper-conference"><strong><font color="blue">AAAI 2024</font></strong></p>
                        <div class="paper-links">
                            <a href="https://dreamidentity.github.io/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-DreamIdentity-yellow"></a>
                        </div>
                    </div>
                </div>

                <h3 class="year-heading">In the Year of 2023 & 2022:</h3>
                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Towards_Accurate_Image_Coding_Improved_Autoregressive_Image_Generation_With_Dynamic_CVPR_2023_paper.pdf" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">Towards Accurate Image Coding: Improved Autoregressive Image Generation With Dynamic Vector Quantization <font color="red">(Highlight, 2.5% of submitted papers)</font></h4>
                        <p class="paper-authors"><strong>Mengqi Huang</strong>, Zhendong Mao, Zhuowei Chen, Yongdong Zhang</p>
                        <p class="paper-conference"><strong><font color="blue">CVPR 2023</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/CrossmodalGroup/DynamicVectorQuantization" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/CrossmodalGroup/DynamicVectorQuantization"></a>
                            <a href="https://youtu.be/ir60YW9JCjU" target="_blank"><img src="https://img.shields.io/badge/Video-DynamicVQ-red?logo=youtube" alt="Video"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Not_All_Image_Regions_Matter_Masked_Vector_Quantization_for_Autoregressive_CVPR_2023_paper.pdf" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation</h4>
                        <p class="paper-authors"><strong>Mengqi Huang</strong>, Zhendong Mao, Quan Wang, Yongdong Zhang</p>
                        <p class="paper-conference"><strong><font color="blue">CVPR 2023</font></strong></p>
                        <div class="paper-links">
                            <a href="https://github.com/CrossmodalGroup/MaskedVectorQuantization" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/CrossmodalGroup/MaskedVectorQuantization"></a>
                            <a href="https://www.youtube.com/watch?v=o2eyRscEejw" target="_blank"><img src="https://img.shields.io/badge/Video-MaskedVQ-red?logo=youtube" alt="Video"></a>
                        </div>
                    </div>
                </div>

                <div class="paper-item">
                    <div class="paper-meta">
                        <a href="https://arxiv.org/abs/2209.01339" class="pdf-link" target="_blank">
                            <img src="./pics/pdf.gif" alt="PDF"> PDF
                        </a>
                    </div>
                    <div class="paper-content">
                        <h4 class="paper-title">DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation <font color="red">(Best Student Paper Award, 1/3009 of submitted papers)</font></h4>
                        <p class="paper-authors"><strong>Mengqi Huang</strong>, Zhendong Mao, Penghui Wang, Quan Wang, Yongdong Zhang</p>
                        <p class="paper-conference"><strong><font color="blue">ACM Multimedia 2022</font></strong></p>
                    </div>
                </div>
            </section>

            <section class="awards">
                <h2>Awards</h2>
                <ul class="awards-list">
                    <li>‰∏≠ÂõΩÂõæË±°ÂõæÂΩ¢Â≠¶Â≠¶‰ºöËá™ÁÑ∂ÁßëÂ≠¶Â•ñ, ‰∏ÄÁ≠âÂ•ñ, 2025</li>
                    <li>‰∏≠ÂõΩÁßëÂ≠¶Èô¢Èô¢ÈïøÁâπÂà´Â•ñ, 2025</li>
                    <li>Best Student Paper Award, ACM Multimedia 2022 (First Author)</li>
                </ul>
            </section>

            <section class="internships">
                <h2>Internships</h2>
                <div class="timeline-item">
                    <div class="timeline-logo">
                        <img src="./pics/bytedance.jpeg" height="24" alt="ByteDance Logo">
                    </div>
                    <div class="timeline-content">
                        <h3 class="timeline-title"><a href="https://bytedance.com/" target="_blank">ByteDance Inc.</a></h3>
                        <p class="timeline-position">Research Intern, Intelligent Creation Department</p>
                        <p class="timeline-location">Beijing</p>
                        <p class="timeline-period">July 2023 - Now</p>

                        <div class="project-section">
                            <div class="project-card">
                                <div class="project-image">
                                    <img src="./pics/teaser_realcustom.jpg" width="100%" alt="RealCustom">
                                </div>
                                <div class="project-text">
                                    <h4 class="project-title">RealCustom: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</h4>
                                    <p><strong><font color="blue">CVPR 2024 & IEEE T-PAMI</font></strong></p>
                                    <p class="project-authors"><strong>Mengqi Huang</strong>, Zhendong Mao, Mingcong Liu, Qian He, Yongdong Zhang</p>
                                    <div class="project-links">
                                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_RealCustom_Narrowing_Real_Text_Word_for_Real-Time_Open-Domain_Text-to-Image_Customization_CVPR_2024_paper.pdf" target="_blank"><img alt="CVPR2024" src="https://img.shields.io/badge/CVPR2024-RealCustom-b31b1b.svg"></a>
                                        <a href="https://pubmed.ncbi.nlm.nih.gov/41105542/" target="_blank"><img alt="TPAMI" src="https://img.shields.io/badge/TPAMI-RealCustom++-b31b1b.svg"></a>
                                        <a href="https://github.com/bytedance/RealCustom" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/bytedance/RealCustom"></a>
                                        <a href="https://corleone-huang.github.io/RealCustom_plus_plus/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-RealCustom-yellow"></a>
                                    </div>
                                    <p class="project-desc">We present RealCustom to disentangle subject similarity from text controllability and thereby allows both to be optimized simultaneously without conflicts. The core idea of RealCustom is to represent given subjects as real words that can be seamlessly integrated with given texts, and further leveraging the relevance between real words and image regions to disentangle visual condition from text condition.</p>
                                </div>
                            </div>
                        </div>

                        <div class="project-section">
                            <div class="project-card">
                                <div class="project-image">
                                    <img src="./pics/teaser_uno.jpg" width="100%" alt="UNO">
                                </div>
                                <div class="project-text">
                                    <h4 class="project-title">Less-to-More Generalization: Unlocking More Controllability by In-Context Generation</h4>
                                    <p><strong><font color="blue">ICCV 2025</font></strong></p>
                                    <p class="project-authors">Shaojin Wu, <strong>Mengqi Huangüìß(Corresponding Author)</strong>, Wenxu Wu, Yufeng Cheng, Fei Ding, Qian He</p>
                                    <div class="project-links">
                                        <a href="https://arxiv.org/abs/2504.02160" target="_blank"><img alt="ICCV2025 paper" src="https://img.shields.io/badge/ICCV2025%20paper-UNO-b31b1b.svg"></a>
                                        <a href="https://github.com/bytedance/UNO" target="_blank"><img alt="GitHub Stars" src="https://img.shields.io/github/stars/bytedance/UNO"></a>
                                        <a href="https://bytedance.github.io/UNO/" target="_blank"><img alt="Project Page" src="https://img.shields.io/badge/Project%20Page-UNO-yellow"></a>
                                    </div>
                                    <p class="project-desc">We propose a highly-consistent data synthesis pipeline for single-subject and multi-subject driven generation. This pipeline harnesses the intrinsic in-context generation capabilities of diffusion transformers and generates high-consistency multi-subject paired data. Moreover, we introduce UNO, which consists of progressive cross-modal alignment and universal rotary position embedding.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-logo">
                        <img src="./pics/kwai-logo.jpeg" height="24" alt="Kuaishou Logo">
                    </div>
                    <div class="timeline-content">
                        <h3 class="timeline-title"><a href="https://www.kuaishou.com/" target="_blank">Kuaishou Technology</a></h3>
                        <p class="timeline-position">Research Intern, Search Technology Department</p>
                        <p class="timeline-location">Beijing</p>
                        <p class="timeline-period">March 2022 - November 2022</p>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-logo">
                        <img src="./pics/mcmaster.jpeg" height="24" alt="McMaster University Logo">
                    </div>
                    <div class="timeline-content">
                        <h3 class="timeline-title"><a href="https://www.mcmaster.ca/" target="_blank">McMaster University</a></h3>
                        <p class="timeline-position">Research Intern, Computing & Software School (Department)</p>
                        <p class="timeline-location">Hamilton, Canada</p>
                        <p class="timeline-period">June 2019 - September 2019</p>
                    </div>
                </div>
            </section>

            <section class="competitions">
                <h2>Competitions</h2>
                <div class="competition-item">
                    <h3 class="competition-title"><a href="https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af50464a0ed647faca6266&award=1,000,000" target="_blank">Á¨¨‰∫åÂ±äÁ≤§Ê∏ØÊæ≥Â§ßÊπæÂå∫ÂõΩÈôÖÁÆóÊ≥ïÁÆó‰æãÂ§ßËµõ-È´òÊïàÂèØÊéßÁöÑÊñáÁîüÂõæÊñπÊ≥ï</a></h3>
                    <p class="competition-role">Team Leader, Second Prize.</p>
                    <p class="competition-period">August 2023 - November 2023</p>
                </div>

                <div class="competition-item">
                    <h3 class="competition-title"><a href="http://www.aiinnovation.com.cn/#/" target="_blank">È¶ñÂ±äÂÖ¥Êô∫ÊùØÂÖ®ÂõΩ‰∫∫Â∑•Êô∫ËÉΩÂ∫îÁî®ÂàõÊñ∞Â§ßËµõ-Â§öÊ®°ÊÄÅÊäÄÊúØÂàõÊñ∞Ëµõ-Âü∫‰∫éÊñáÊú¨ÁöÑÂõæÂÉèÁîüÊàê</a></h3>
                    <p class="competition-role">Team Leader, Second Prize.</p>
                    <p class="competition-period">August 2022 - November 2022</p>
                </div>

                <div class="competition-item">
                    <h3 class="competition-title"><a href="http://smp-challenge.com/" target="_blank">ACM Multimedia 2020 Social Media Prediction Challenge</a></h3>
                    <p class="competition-role">Team Leader, Top Performance Award.</p>
                    <p class="competition-links"><a href="https://github.com/Corleone-Huang/Social-Media-Popularity-Prediction-Challenge-2020" target="_blank">[Github]</a></p>
                    <p class="competition-period">March 2020 - June 2020</p>
                </div>
            </section>

            <div class="visitor-tracker">
                <a href="https://clustrmaps.com/site/1bpao" title="Visit tracker">
                    <img src="http://clustrmaps.com/map_v2.png?d=x9zZiIk7-mY5aFcqqnSdkP6fOQeNgwkz4dAD8Esq9hE&cl=ffffff" height="100" alt="Visitor Map">
                </a>
            </div>

            <footer class="page-footer">
                <p>Last update: October, 2025. Webpage template borrows from <a href="http://wnzhang.net/">Weinan Zhang</a>.</p>
            </footer>
        </main>

        <aside id="news">
            <h2>News</h2>
            <div class="news-item">
                <p class="news-date"><strong>November 2025</strong></p>
                <p class="news-content">1 paper is accepted by AAAI 2026</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>October 2025</strong></p>
                <p class="news-content">1 paper is accepted by T-PAMI!!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>September 2025</strong></p>
                <p class="news-content">1 paper is accepted by NeurIPS 2025!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>June 2025</strong></p>
                <p class="news-content">4 papers are accepted by ICCV 2025!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>February 2025</strong></p>
                <p class="news-content">4 papers are accepted by CVPR 2025!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>December 2024</strong></p>
                <p class="news-content">1 papers is accepted by AAAI 2025!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>February 2024</strong></p>
                <p class="news-content">1 papers is accepted by CVPR 2024!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>December 2023</strong></p>
                <p class="news-content">2 papers are accepted by AAAI 2024!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>March 2023</strong></p>
                <p class="news-content">Our paper "Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization" is selected as a <font color="red">highlight</font> at CVPR 2023!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>February 2023</strong></p>
                <p class="news-content">2 papers are accepted by CVPR 2023!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>October 2022</strong></p>
                <p class="news-content">Our paper "DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation" receives the <font color="red">Best Student Paper Award</font> at ACM Multimedia 2022!</p>
            </div>
            
            <div class="news-item">
                <p class="news-date"><strong>June 2022</strong></p>
                <p class="news-content">1 paper is accepted by ACM Multimedia 2022!</p>
            </div>
        </aside>
    </div>
</body>
</html>